# Group 05 — Becks

## Week 1 — Modelling
Please open [modelling.pdf](https://git.uni-jena.de/fusion/teaching/project/2025sose/KnowledgeGraphs/group-05/-/blob/main/modelling.pdf)

## Week 2 — Create KG from Tabular Data
The pipeline works as follows:

1. Extract ingredients from the dataset (tabular data) using a large language model `create_batch.py`, `upload_batch.py`
2. Download the results using `get_batch_results.py`
3. Clean the LLMs results by manually removing/mapping some common mistakes to the correct category, e.g. "banana pepper" to "bell pepper"
4. Map the ingredients to Wikidata items using elastic search and a SPARQL query `ingredient_QID_mapping.py`
5. Extract cities from the dataset and map them to Wikidata items `city_QID_mapping.py`
6. Use `rdflib` to integrate the three data sources into statements within the ontology `integrate_tabular_data_with_ontology.py`
   1. Add the pizza places with schema.org-addresses
   2. Create the pizzas
      1. The ontology has existing ingredients. Again override the mapping to the entities from within the ontology using
           `KNOWN_INGREDIENTS`
      2. Add these ingredients (hierarchy: use ontology ingredients → use wikidata items → create canonical ingredient in the ontology)
      3. Add the price
   3. Write the results to `pizza_data.ttl`


### TL;DR
### Ingredients
1. Run
```shell
python3 upload_batch.py
```
2. Run
```shell
python3 get_batch_results.py
```  
3. Run
```shell
python3 clean_llm_results.py
```
4. Fill the `locked_qid_map.json` with expert knowledge
5. Run
```shell
python3 ingredient_QID_mapping.py 
```
6. Run
```shell
python3 create_ingredients.jsonl.py
```

### Cities
1. Fill the `locked_city_qid_map.json` with "expert" knowledge
2. Run
```shell
python3 city_qid_mapping.py
```

### Integrate Data from CSV-file with Ontology
```shell
python3 integrate_tabular_data_with_ontology.py 
```

### File Purposes

| File name                  | Purpose                                                                                                                    |
|----------------------------|----------------------------------------------------------------------------------------------------------------------------|
| `results_k.json`           | The ingredient extraction step output from the LLM                                                                         |
| `data.csv`                 | The dataset given from the task                                                                                            |
| `ingredient_qid_map.json`  | Defines the mapping from normalised ingredient name to a Wikidata item, generated by `ingredient_QID_mapping.py`           |
| `locked_qid_map.json`      | Contains manual mappings for ingredients to a Wikidata item                                                                |
| `city_qid_map.json`        | In analogy to ingredient_qid_map, it defines the mapping from city name to a Wikidata item, gen'd by `city_qid_mapping.py` |
| `locked_city_qid_map.json` | Same as `locked_qid_map.json`, but now for cities                                                                          |
| `ingredients.jsonl`        | Generated by `create_ingredients.jsonl.py` from a `results.json` (the output from the LMM ingredient extraction step       |



### Possible improvements
+ It is obvious that the ontology is in German and the data is English, and it would be nice to unify them
    We made an informed decision to keep them different, to show that they can be integrated, even though different
    languages were used. Furthermore, we could use this for other cases, were different, but equivalently legitimate 
    terminology was used.
+ Add place categories to Pizzeria, like "Burger Place", "University", ...
+ The list of KNOWN_INGREDIENTS should only contain ingredients which are not present in Wikidata. 
    Furthermore, it would be even better to add the missing ingredients to Wikidata, instead of
    defining them in our ontology. But this script's purpose it to show what can be done, not the real deal.
+ The ingredients could be further categorised, e.g. "sun-dried tomatoes", "cherry tomatoes" and "tomato basil sauce"
    could be subsumed in a class "tomato ingredients". The same applies to "fried egg". If the goal is to keep the 
    querying simple, one could add the class itself as an ingredient assertion. 
+ We could, instead of creating our own ontology, use `schema.org` for any assertion.
+ The LLM extraction step can be improved with more prompt engineering
+ The LLM cleaning step can be improved with classical NLP methods, e.g. the word
   stems or singulars could be used as normalised names or for matching a Wikidata item

```turtle
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .
@prefix ex: <http://example.org/> .
@prefix schema: <http://schema.org/> .

ex:Store1 a schema:FoodEstablishment ;
  schema:hasMenuItem ex:MenuItem1 .

ex:MenuItem1 a schema:MenuItem ;
  schema:name "Cheeseburger" ;
  schema:price "5.99"^^xsd:decimal ;
  schema:priceCurrency "USD" .
```