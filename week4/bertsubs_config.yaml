# Copyright 2023 Jiaoyan Chen. All rights reserved.

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# @paper(
#     "Contextual Semantic Embeddings for Ontology Subsumption Prediction (World Wide Web Journal)",
# )
model: bertsubs

src_onto_file: null
tgt_onto_file: null

# More than one label properties can be added for src_label_property and tgt_label_property
src_label_property:
  - http://www.w3.org/2000/01/rdf-schema#label
tgt_label_property:
  - http://www.w3.org/2000/01/rdf-schema#label
use_one_label: true
subsumption_type: named_class

# test_subsumption_file is optional. If it is set to None (or left as empty), the output is the fine-tuned model (and its validation results);
#                                    otherwise, the output is the find-tuned model, (its validation results) and the testing results.
# test_type: "prediction" (predict scores for candidate subsumptions given in test_submission_file
#            or "evaluation" (calculate metrics with ground truths given in the test_subsumption_file)
test_subsumption_file: null
test_type: prediction

# not do entailment reasoning
no_reasoning: false

# valid_subsumption_file is optional. If it is set, validation metrics are calculated and output.
valid_subsumption_file: null

# train_subsumption_file is optional. If it is set, the samples will be added for fine-tuning
train_subsumption_file: null
use_ontology_subsumptions_training: true

prompt:
  prompt_type: isolated # "isolated", "traversal", "path"
  prompt_hop: 1
  prompt_max_subsumptions: 4
  use_sub_special_token: false # true: <SUB>, false: <SEP> (only works for prompt_type of "path")
  context_dup: 4
  max_length: 128

fine_tune:
  do_fine_tune: true
  pretrained: bert-base-uncased    # already fine-tuned or pre-trained model can be used by setting do_fine_tune to false
  tokenizer": bert-base-uncased
  output_dir: fine-tuned-bert
  warm_up_ratio: 0.0
  num_epochs: 5.0
  early_stop: false
  train_pos_dup: 2
  train_neg_dup: 2
  batch_size: 32

evaluation:
  batch_size: 32

global_matching:
  enabled: true
  num_raw_candidates: 1000
  num_best_predictions: 20
  mapping_extension_threshold: 0.5 # May not be used by BERTSubs, but good to have
  mapping_filtered_threshold: 0.8 # Subsumption is stricter, use a higher threshold
  for_oaei: false
