# Week 2 — Create KG from Tabular Data

You will need an OpenAI API Key to run the code. Store it in the environment variable `OPENAI_API_KEY` or change the
code to pass it to the client when instantiating it.

## The Pipeline

The new pipeline works as follows:
1. Extract ingredients from the tabular data using a large language model, to get rid of irrelevant information. [create_batch.py](create_batch.py), [upload_batch.py](upload_batch.py), [get_batch_results.py](get_batch_results.py)
2. Create a hierarchical clustering of the ingredients. Similar ingredients should be put into the same category, e.g. "tomato" and "sun-dried tomato".
3. Map the ingredients to my Ontology or Wikidata.
   1. The ontology has existing ingredients. Again override the mapping to the entities from within the ontology
          using `KNOWN_INGREDIENTS`
   2. Add these ingredients (hierarchy: use ontology ingredients → use wikidata items → create canonical ingredient
      in the ontology)
4. Map cities to the mentioned locations in the dataset.
5. Lastly, integrate the tabular data with the ontology, by creating pizza places, pizzas and ingredients.
6. Write the results to [pizza_data.ttl](pizza_data.ttl)

## Commands for the Pipeline
### Ingredients

1. Run

```shell
python3 create_batch.py
python3 upload_batch.py
python3 get_batch_results.py
python3 clean_llm_results.py
```  

2. Fill the [locked_qid_map.json](locked_qid_map.json) with expert knowledge
3. Run

```shell
python3 create_ingredients.jsonl.py 
python3 ingredient_QID_mapping.py
python3 copy_pizza_ingredients_to_clean_json.py
python3 clustering.py
```


### Cities

1. Fill the [locked_city_qid_map.json](locked_city_qid_map.json) with "expert" knowledge
2. Run

```shell
python3 city_qid_mapping.py
```

### Integrate Data from CSV-file with Ontology

```shell
python3 integrate_tabular_data_with_ontology.py # creates pizza_data.ttl
```

## File Purposes

| File name                                            | Purpose                                                                                                                                                         |
|------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [results_k.json](results_k.json)                     | The ingredient extraction step output from the LLM                                                                                                              |
| [data.csv](data.csv)                                 | The dataset given from the task                                                                                                                                 |
| [ingredient_qid_map.json](ingredient_qid_map.json)   | Defines the mapping from normalised ingredient name to a Wikidata item, generated by [ingredient_QID_mapping.py](ingredient_QID_mapping.py)                     |
| [locked_qid_map.json](locked_qid_map.json)           | Contains manual mappings for ingredients to a Wikidata item                                                                                                     |
| [city_qid_map.json](city_qid_map.json)               | In analogy to ingredient_qid_map, it defines the mapping from city name to a Wikidata item, gen'd by [city_qid_mapping.py](city_qid_mapping.py)                 |
| [locked_city_qid_map.json](locked_city_qid_map.json) | Same as [locked_qid_map.json](locked_qid_map.json), but now for cities                                                                                          |
| [ingredients.jsonl](ingredients.jsonl)               | Generated by [create_ingredients.jsonl.py](create_ingredients.jsonl.py) from a [results.json](results.json) (the output from the LMM ingredient extraction step |
| [pizza_data.ttl](pizza_data.ttl)                     | The results of the integration step, containing the pizza places, pizzas and ingredients, as well as the prices and addresses                                   |

## Possible improvements

+ It is obvious that the ontology is in German and the data is English, and it would be nice to unify them
  We made an informed decision to keep them different, to show that they can be integrated, even though different
  languages were used. Furthermore, we could use this for other cases, were different, but equivalently legitimate
  terminology was used. A translation step could be added to the pipeline, but it is beyond the scope of this task.

+ Add place categories to Pizzeria, like "Burger Place", "University", ...
+ The LLM extraction step can be improved with more prompt engineering
+ The LLM cleaning step can be improved with classical NLP methods, e.g. the word
  stems or singulars could be used as normalised names or for matching a Wikidata item

--- 

# Actual Task: Tabular Data

Transform Pizza_data into RDF triples using OntotextRefine and/or your favorite programming language. Please document your code (e.g., by exporting the workflow from Ontotext Refine). Save the RDF data into turtle format (.ttl) in your gitlab and upload it to your GraphDB instance.

Subtask RDF.1 Discuss in the report the different transformation choices and how entity resolution was treated (30%). As we saw in the module, there is not a unique way to transform the elements in a table into elements of a knowledge graph (i.e., classes, object properties, data properties or instances). For example the column “menu item” contains the name of the pizza but it may also include additional information about toppings and type of pizza. Tip1: You will need to apply some basic text processing and entity recognition based on the ontology vocabulary (e.g. using Ontotext Refines capabilities or by writing some code). Tip2: After processing the data, one may also need to extend the ontology with new elements.

(*)Subtask RDF.2 Create RDF triples (30%). Use the created ontology to guide the transformation e.g., to link the data to the ontology (e.g., via rdf:type) and to use the defined ontology properties (e.g., has_topping) and concepts (Margherita_Pizza).

(*)Subtask RDF.3 For the cells in the columns city, country and state; instead of creating new URIs (e.g., new individuals) for the information in the table cells, reuse an entity URI from DBPedia, Wikidata or Google’s Knowledge Graph (http:// dbpedia.org/resource/Los_Angeles). Tip: communicate with their respective look-up services. (Again using Ontotext Refine or by programming something (for this, you may want to check the City Uni lab on this topic at https://github.com/turing-knowledge-graphs/teaching/blob/main/city/2023-2024/IN3067-INM713_Lab_Session5_CSV2KG_with_solutions.pdf (25%).

Subtask RDF.4 Correctness of the code and code documentation (15%).