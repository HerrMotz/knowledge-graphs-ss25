# Week 2 — Create KG from Tabular Data

You will need an OpenAI API Key to run the code. Store it in the environment variable `OPENAI_API_KEY` or change the
code to pass it to the client when instantiating it.

## The Pipeline

The new pipeline works as follows:
1. Extract ingredients from the tabular data using a large language model, to get rid of irrelevant information. [create_batch.py](create_batch.py), [upload_batch.py](upload_batch.py), [get_batch_results.py](get_batch_results.py)
2. Create a hierarchical clustering of the ingredients. Similar ingredients should be put into the same category, e.g. "tomato" and "sun-dried tomato".
3. Map the ingredients to my Ontology or Wikidata.
   1. The ontology has existing ingredients. Again override the mapping to the entities from within the ontology
          using `KNOWN_INGREDIENTS`
   2. Add these ingredients (hierarchy: use ontology ingredients → use wikidata items → create canonical ingredient
      in the ontology)
4. Map cities to the mentioned locations in the dataset.
5. Lastly, integrate the tabular data with the ontology, by creating pizza places, pizzas and ingredients.
6. Write the results to [pizza_data.ttl](pizza_data.ttl)

## Commands for the Pipeline
### Ingredients

1. Run

```shell
python3 create_batch.py
python3 upload_batch.py
python3 get_batch_results.py
python3 clean_llm_results.py
```  

2. Fill the [locked_qid_map.json](locked_qid_map.json) with expert knowledge
3. Run

```shell
python3 create_ingredients.jsonl.py 
python3 ingredient_QID_mapping.py
python3 copy_pizza_ingredients_to_clean_json.py
python3 clustering.py
```


### Cities

1. Fill the [locked_city_qid_map.json](locked_city_qid_map.json) with "expert" knowledge
2. Run

```shell
python3 city_qid_mapping.py
```

### Integrate Data from CSV-file with Ontology

```shell
python3 integrate_tabular_data_with_ontology.py # creates pizza_data.ttl
```

## File Purposes

| File name                                            | Purpose                                                                                                                                                         |
|------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [results_k.json](results_k.json)                     | The ingredient extraction step output from the LLM                                                                                                              |
| [data.csv](data.csv)                                 | The dataset given from the task                                                                                                                                 |
| [ingredient_qid_map.json](ingredient_qid_map.json)   | Defines the mapping from normalised ingredient name to a Wikidata item, generated by [ingredient_QID_mapping.py](ingredient_QID_mapping.py)                     |
| [locked_qid_map.json](locked_qid_map.json)           | Contains manual mappings for ingredients to a Wikidata item                                                                                                     |
| [city_qid_map.json](city_qid_map.json)               | In analogy to ingredient_qid_map, it defines the mapping from city name to a Wikidata item, gen'd by [city_qid_mapping.py](city_qid_mapping.py)                 |
| [locked_city_qid_map.json](locked_city_qid_map.json) | Same as [locked_qid_map.json](locked_qid_map.json), but now for cities                                                                                          |
| [ingredients.jsonl](ingredients.jsonl)               | Generated by [create_ingredients.jsonl.py](create_ingredients.jsonl.py) from a [results.json](results.json) (the output from the LMM ingredient extraction step |
| [pizza_data.ttl](pizza_data.ttl)                     | The results of the integration step, containing the pizza places, pizzas and ingredients, as well as the prices and addresses                                   |

## Possible improvements

+ It is obvious that the ontology is in German and the data is English, and it would be nice to unify them
  We made an informed decision to keep them different, to show that they can be integrated, even though different
  languages were used. Furthermore, we could use this for other cases, were different, but equivalently legitimate
  terminology was used. A translation step could be added to the pipeline, but it is beyond the scope of this task.

+ Add place categories to Pizzeria, like "Burger Place", "University", ...
+ The LLM extraction step can be improved with more prompt engineering
+ The LLM cleaning step can be improved with classical NLP methods, e.g. the word
  stems or singulars could be used as normalised names or for matching a Wikidata item
